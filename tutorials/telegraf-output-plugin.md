# PostgreSQL and TimescaleDB output plugin for Telegraf

Telegraf can collect metrics from a wide array of inputs and write them into a wide array of outputs. It is plugin-driven for both collection and output of data so it is easily extendable. It is written in Go, which means that it is a compiled and standalone binary that can be executed on any system with no need for external dependencies, no npm, pip, gem, or other package management tools required. 

Telegraf is an open-source tool. It contains over 200 plugins for gathering and writing different types of data written by people who work with that data. 
As an open-source project hosted on GitHub it has it's process of a Pull Request review. When a plugin is written it is submitted for a review by the developers of Telegraf. 
If accepted the new code is merged and the plugin is available with the following release. 

We wrote the PostgreSQL output plugin which also has the ability to send data to an TimescaleDB hypertable. The Pull Request is opened and currently under review by the Telegraf developers. 

This tutorial will show a couple of examples on how to use the PostgreSQL/TimescaleDB output plugin for Telegraf, and while the code is in review also show how to download a built binary of Telegraf with our plugin already included.

## Installation [](telegraf-installation)

### Before we start

Before we start, you will need [TimescaleDB installed][getting-started] and a means to connect to it

### Setting up Telegraf

Telegraf is written in Go, so only one standalone binary is required to run it. But also because of this all the code for the different plugins must be part of that binary. We have an unoficial build of Telegraf version 1.10.4 with our plugin added. You can download from the following links: 

* Linux amd64 [deb build] [rpm build] [binary]
* Windows amd64 [binary/exe]
* MacOS amd64 [binary]

We can also provide you with a builds for:

* Windows i386
* Linux (i386, armhf, armel, arm64, static_amd64, s390x, mipsel
* FreeBSD (amd64, i386)

if you contact us at the our [Public Support Slack][public-slack]

Once you download the binary and extract it to a suitable location (or install the packages) we can test out the build. 
We can check the version of the installed Telegraf with

```
$ telegraf --version
```

If the installation was good it should print out `Telegraf 1.10.4withPG`.

## Telegraf Configuration [](telegraf-configuration)

When Telegraf is started, a config file needs to be specified. 
The config file contains the setup for the:
* Telegraf agent
  * Collection interval
  * Jitter
  * Buffer and batch size and so on
* Global tags added to all collected metrics from all inputs
* Enabled outputs, processors, aggreagators, inputs (and their respective configuration)

A sample config file can be generated by Telegraf itself.
If you execute 

```
$ telegraf config > telegraf.conf
```

The `telegraf.conf` file will be generated. By default it creates a configuration 
that collects data every 10 seconds, has enabled the input plugins for CPU, Memory, Disk IO and other devops related measurements. The default enabled output plugin is the Influx plugin. 

In the same file sample configuration is added  for all the available input, output, processor and aggregator plugins, **but commented out**. So it's easy to see how a plugin is to be configured.

But Telegraf offers ways to generate a config file with only the required plugins. 
If you execute

```
$ ./telegraf --input-filter=cpu --output-filter=postgresql config > telegraf.conf
```

You select only the CPU input plugin and the PostgreSQL ouptut plugin to be enabled 
in the generated `telegraf.conf` file. The file still has the processors and aggregators commented out. 

The enabled input plugin will sample various metrics about the usage of the CPU.

### Testing out the config file

The selected input plugin and agent settings can be tested so they output 
a single collection to STDOUT. By running 

```
$ ./telegraf --config telegraf.conf --test
```

we select the previously generated config file that enables only the CPU input plugin.
And the output should look something like: 

```
> cpu,cpu=cpu0,host=local usage_guest=0,usage_idle=78.431372,usage_iowait=0,usage_irq=0,usage_softirq=0,usage_steal=0,usage_system=11.764705,usage_user=9.803921 1558613882000000000
> cpu,cpu=cpu1,host=local usage_guest=0,usage_idle=92.156862,usage_iowait=0,usage_irq=0,usage_softirq=0,usage_steal=0,usage_system=3.921568,usage_user=3.921568 1558613882000000000
> cpu,cpu=cpu-total,host=local usage_guest=0,usage_idle=87.623762,usage_iowait=0,usage_irq=0,usage_softirq=0,usage_steal=0,usage_system=6.435643,usage_user=5.940594 1558613882000000000
```

A line is outputed for each core of the CPU and the total. Values are presented in `key=value` pairs with the timestamp last in the row. 
Writing to STDOUT doesn't distinguish between *tags*, which are indexed fields (cpu, host) and value *fields* (usage_quest, usage_user ...). 
The distinction exists because different configuration options available for the different fields.

### Configuring the PostgreSQL Output Plugin

The `telegraf.conf` file we generated has a section (around line 80) headered with 

```
################################################
#                OUTPUT PLUGINS                #
################################################
```

And below this header, the default configuration for the PostgreSQL output plugin is
laid out. 

```
[[outputs.postgresql]]
  ## specify address via a url matching:
  ##   postgres://[pqgotest[:password]]@localhost[/dbname]\
  ##       ?sslmode=[disable|verify-ca|verify-full]
  ## or a simple string:
  ##   host=localhost user=pqotest password=... sslmode=... dbname=app_production
  ##  
  ## All connection parameters are optional.
  ##  
  ## Without the dbname parameter, the driver will default to a database
  ## with the same name as the user. This dbname is just for instantiating a
  ## connection with the server and doesn't restrict the databases we are trying
  ## to grab metrics for.
  ##  
  address = "host=localhost user=postgres sslmode=verify-full"

  ## Store tags as foreign keys in the metrics table. Default is false.
  # tags_as_foreignkeys = false

  ## Template to use for generating tables
  ## Available Variables:
  ##   {TABLE} - tablename as identifier
  ##   {TABLELITERAL} - tablename as string literal
  ##   {COLUMNS} - column definitions
  ##   {KEY_COLUMNS} - comma-separated list of key columns (time + tags)
  ## Default template
  # table_template = "CREATE TABLE IF NOT EXISTS {TABLE}({COLUMNS})"
  ## Example for timescaledb
  # table_template = "CREATE TABLE {TABLE}({COLUMNS}); SELECT create_hypertable({TABLELITERAL},'time');" 

  ## Schema to create the tables into
  # schema = "public"

  ## Use jsonb datatype for tags
  # tags_as_jsonb = false
  ## Use jsonb datatype for fields
  # fields_as_jsonb = false
```

From the config we can notice several things:
1. The top line enables the plugin, the plugin specific config is indented after this line
2. There is currently only one parameter configured, `address`. The others are commented out
3. Possible parameters are commented out with a single `#`. (tags_as_foreignkeys, table_template, schema, tags_as_jsonb, fields_as_jsonb)
4. Explanations of the parameters are commented out with a single `##`

The commented parameters also show the default values they have when commented out.

For the first example we'll set up the address parameter to a propper connection string
so a connection to an instance of TimescaleDb or PostgreSQL can be established. All the other parameters will have their default values.

## Running Telegraf

When we run telegraf we only need to specify the config file to be used. If we execute 

```
$  telegraf --config telegraf.conf 
2019-05-23T13:48:09Z I! Starting Telegraf 1.10.4withPG
2019-05-23T13:48:09Z I! Loaded inputs: cpu
2019-05-23T13:48:09Z I! Loaded outputs: postgresql
2019-05-23T13:48:09Z I! Tags enabled: host=local
2019-05-23T13:48:09Z I! [agent] Config: Interval:10s, Quiet:false, Hostname:"local", Flush Interval:10s
```

In the output you can notice the loaded inputs (cpu) and outputs (postgresql) along with the global tags and the intervals with which the agent will collect the data from the inputs, and flush to the outputs. We can stop the execution of Telegraf after ~10-15 seconds.

Let us now connect to our PostgreSQL instance and look around the data

```
$ psql -U postgres -h localhost
```

The cpu input plugin has one measurement, called cpu, and it's stored in a table of the same name (by default in the public schema).
So with the SQL query `SELECT * FROM cpu`, depending on how long you left Telegraf running you will see the table populated with some values, and we can find the average usage per cpu core with `SELECT cpu, avg(usage_user) FROM cpu GROUP BY cpu` and the output would look like 

```
    cpu    |       avg        
-----------+------------------
 cpu-total | 8.46385703620795
 cpu0      | 12.4343351351033
 cpu1      | 4.88380203380203
 cpu2      | 12.2718724052057
 cpu3      | 4.26716970050303
```

### Creating hypertables

The plugin we developped allows the user to configure several parameters. The `table_template` parameter defines the SQL to be executed when a new measurement is recorded by Telegraf and the
required table doesn't exist in the output database. By default the `table_template` used is `CREATE TABLE IF NOT EXISTS {TABLE}({COLUMNS})` where `{TABLE}` and `{COLUMNS}` are placeholders 
for the name of the table and the column definitions.

For users of TimescaleDB they can update the `table_template` parameter in the config with
```
  table_template=`CREATE TABLE IF NOT EXISTS {TABLE}({COLUMNS}); SELECT create_hypertable({TABLELITERAL},'time',chunk_time_interval := '1 week'::interval,if_not_exists := true);`
```

This way when a new table is created it is converted into a hypertable, with each chunk holding 1 week intervals. Nothing else is needed to use the same plugin with TimescaleDB. 

### Adding new Tags or Fields

Your telegraf configuration can change at any moment. An input plugin can be reconfigured to produce different data, or you may decide to index your data with different tags. The plugin we built can dynamicly update the created tables with new columns as they appear. The previous configuration we used had no global tags specified other than the `host` tag. We will now add a new global tag in the configuration. Open the file in any text editor and update the `[global_tags]` section (around line 18) with: 

```
[global_tags]
  location="New York"
```

This way all metric collected with the instance of telegraf running with this config will be tagged with location="New York". If we run Telegraf again, collecting the metrics in TimescaleDB

```
$ telegraf --config telegraf.conf
```

And after a while we check on the `cpu` table in the database

```
psql> \dS cpu
\dS cpu;
Table "public.cpu"
      Column      |           Type           
------------------+--------------------------
 time             | timestamp with time zone 
 cpu              | text                     
 host             | text                     
 usage_steal      | double precision         
 usage_iowait     | double precision         
 usage_guest      | double precision         
 usage_idle       | double precision         
 usage_softirq    | double precision         
 usage_system     | double precision         
 usage_user       | double precision         
 usage_irq        | double precision         
 location         | text                     
 ```

 The `location` column was added and it contains "New York" for all rows.

 ### Separate table for the Tags

 The plugin we developed allows the user to select to have the tag sets inserted in a separate
 table and then referenced via Foreign Key in the measurement table. This way space can be saved
 for measures with low cardinality tag sets. To enable this change, you need to uncomment the `tags_as_foreignkeys` parameter in plugin config (around line  103 in `telegraf.conf`) and set it to true

 ```
 ## Store tags as foreign keys in the metrics table. Default is false.
 tags_as_foreignkeys = true
 ```

 ### JSONb column for Tags and Fields

 Additionally the tags and fields can be stored as JSONb columns in the database. All you need to do is uncomment the `tags_as_jsonb` or `fields_as_jsonb` parameters in `telegraf.conf` (around line 120) and set them to true. In this example we'll store the fields as separate columns, but the tags as JSON. 

 ```
 ## Use jsonb datatype for tags
 tags_as_jsonb = true
 ## Use jsonb datatype for fields
 fields_as_jsonb = false
 ```

To better visualize the result we'll drop the existing `cpu` table from our database.
```
psql> DROP TABLE cpu;
```

Now we'll fire Telegraf up again, this time with the config changed to write the tags in a separate table, and as JSON.
```
$ telegraf --config telegraf.conf
```

We can turn it off after 20-30 seconds. If we check on the `cpu` table in the database:

```
psql> \dS cpu
\dS cpu
Table "public.cpu"
      Column      |           Type           
------------------+--------------------------
 time             | timestamp with time zone 
 tag_id           | integer                  
 usage_irq        | double precision         
 usage_softirq    | double precision         
 usage_system     | double precision         
 usage_iowait     | double precision         
 usage_guest      | double precision         
 usage_user       | double precision         
 usage_idle       | double precision         
 usage_steal      | double precision         
```
We can notice the `cpu`, `host` and `location` columns are not there, instead there's a `tag_id` column. The tag sets are stored in a separate table called `cpu_tag`:
```
 psql> SELECT * FROM cpu_tag;
 tag_id |                                       tags                                        
--------+-----------------------------------------------------------------------------------
      1 | {"cpu": "cpu-total", "host": "local", "location": "New York"}
      2 | {"cpu": "cpu0", "host": "local", "location": "New York"}
      3 | {"cpu": "cpu1", "host": "local", "location": "New York"}
```

And instead of having three text columns populated with the a small number of possible values, only a single integer will be used.

## Next Steps

Once you have started inserting data in TimescaleDB, you can begin to familiarize yourself with our [architecture](https://docs.timescale.com/introduction/architecture) and [API reference](https://docs.timescale.com/api).

Additionally, we have several other [tutorials][] available for you to explore as you become accustomed to working with TimescaleDB.

[getting-started]: /getting-started
[tutorials]: /tutorials
[public-slack]: https://timescaledb.slack.com/